{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.28.1\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW, PreTrainedTokenizer\n",
    "from collections import namedtuple\n",
    "import json\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "#from model import BARTVAEClassifier, BARTDecoderClassifier, BARTVADVAEClassifier, RobertaClassifier\n",
    "#from utils import ErcTextDataset, get_num_classes, get_label_VAD, convert_label_to_VAD, save_latent_params, compute_VAD_pearson_correlation, replace_for_robust_eval\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import yaml\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, \\\n",
    "    precision_recall_fscore_support, precision_score, recall_score\n",
    "import torch.cuda.amp.grad_scaler as grad_scaler\n",
    "import torch.cuda.amp.autocast_mode as autocast_mode\n",
    "from transformers.models.bart.modeling_bart import BartModel, BartDecoder\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForMaskedLM, AutoModel, AutoTokenizer, AutoConfig, BartTokenizer, BartConfig,RobertaForSequenceClassification, BartForSequenceClassification\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding, BartEncoderLayer, BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class EncodedDataset(Dataset):\n",
    "\n",
    "  def __init__(self, input_sents: List[str], \n",
    "                input_labels: List[int], \n",
    "                target_labels:List[int], \n",
    "                encoder_tokenizer: PreTrainedTokenizer,\n",
    "                decoder_tokenizer: PreTrainedTokenizer,\n",
    "                max_sequence_length: int = None, \n",
    "                max_targets: int = 8):\n",
    "      \n",
    "    self.input_sents = input_sents\n",
    "    self.input_labels = input_labels\n",
    "    self.target_labels = target_labels\n",
    "    self.encoder_tokenizer = encoder_tokenizer\n",
    "    self.decoder_tokenizer = decoder_tokenizer\n",
    "    self.max_sequence_length = max_sequence_length\n",
    "    self.max_targets = max_targets\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_sents) \n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    text = self.input_sents[index]\n",
    "    label = self.input_labels[index]\n",
    "    labels = np.zeros(2)\n",
    "    labels[label] = 1\n",
    "    target = self.target_labels[index]\n",
    "    target_labels = np.zeros(8)\n",
    "    #target_labels[target] = 1\n",
    "    target_labels = torch.tensor(target_labels)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    token = self.encoder_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
    "\n",
    "    if self.decoder_tokenizer.pad_token is None:\n",
    "      self.decoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    token = self.decoder_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "    dec_input_ids, dec_mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
    "\n",
    "    return input_ids, mask_ids, target_labels, labels, dec_input_ids, dec_mask_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Inference(nn.Sequential):\n",
    "    def __init__(self, num_input_channels, latent_dim, disc_variable=True):\n",
    "        super(_Inference, self).__init__()\n",
    "        if disc_variable:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, num_input_channels//2))\n",
    "            self.add_module('relu', nn.ReLU())\n",
    "            self.add_module('fc2', nn.Linear(num_input_channels//2, latent_dim))\n",
    "            self.add_module('log_softmax', nn.LogSoftmax(dim=1))\n",
    "        else:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, latent_dim))\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(Sample, self).__init__()\n",
    "        self._temperature = temperature\n",
    "\n",
    "    def forward(self, norm_mean, norm_log_sigma, target_sample, disc_label=None, mixup=False, disc_label_mixup=None,\n",
    "                mixup_lam=None):\n",
    "        \"\"\"\n",
    "        :param norm_mean: mean parameter of continuous norm variable\n",
    "        :param norm_log_sigma: log sigma parameter of continuous norm variable\n",
    "        :param disc_log_alpha: log alpha parameter of discrete multinomial variable\n",
    "        :param disc_label: the ground truth label of discrete variable (not one-hot label)\n",
    "        :param mixup: if we do mixup\n",
    "        :param disc_label_mixup: the mixup target label\n",
    "        :param mixup_lam: the mixup lambda\n",
    "        :return: sampled latent variable\n",
    "        \"\"\"\n",
    "        batch_size = norm_mean.size(0)\n",
    "        latent_sample = list([])\n",
    "        latent_sample.append(self._sample_norm(norm_mean, norm_log_sigma))\n",
    "        latent_sample.append(target_sample)\n",
    "        latent_sample = torch.cat(latent_sample, dim=1)\n",
    "        dim_size = latent_sample.size(1)\n",
    "        latent_sample = latent_sample.view(batch_size, dim_size, 1, 1)\n",
    "        return latent_sample\n",
    "\n",
    "    def _sample_gumbel_softmax(self, log_alpha):\n",
    "        \"\"\"\n",
    "        Samples from a gumbel-softmax distribution using the reparameterization\n",
    "        trick.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_alpha : torch.Tensor\n",
    "            Parameters of the gumbel-softmax distribution. Shape (N, D)\n",
    "        \"\"\"\n",
    "        EPS = 1e-12\n",
    "        unif = torch.rand(log_alpha.size()).cuda()\n",
    "        gumbel = -torch.log(-torch.log(unif + EPS) + EPS)\n",
    "        # Reparameterize to create gumbel softmax sample\n",
    "        logit = (log_alpha + gumbel) / self._temperature\n",
    "        return torch.softmax(logit, dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_norm(mu, log_sigma):\n",
    "        EPS = 1e-6\n",
    "        std_z = torch.randn(mu.size()).to(mu.device)\n",
    "        sigma = torch.exp(torch.clamp(log_sigma, max=10))  # Clamp to prevent overflow\n",
    "        return mu + sigma * std_z\n",
    "    # staticmethod\n",
    "    # def _sample_norm(mu, log_sigma):\n",
    "    #     \"\"\"\n",
    "    #     :param mu: the mu for sampling with N*D\n",
    "    #     :param log_sigma: the log_sigma for sampling with N*D\n",
    "    #     Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "    #     \"\"\"\n",
    "    #     std_z = torch.randn(mu.size())\n",
    "    #     if mu.is_cuda:\n",
    "    #         std_z = std_z.cuda()\n",
    "\n",
    "    #     return mu + torch.exp(log_sigma) * std_z\n",
    "\n",
    "\n",
    "class HATE_WATCH(nn.Module):\n",
    "    \"\"\"The VAD-VAE model.\"\"\"\n",
    "    def __init__(self, temperature,gamma,eta,enc_chk,check_point, dec_chk,bart_check_point,num_targets, num_class, beta_c,beta_d, device, batch_size, decoder_type, x_sigma=1):\n",
    "        super(HATE_WATCH, self).__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_type = decoder_type\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature, requires_grad=True, device=device))\n",
    "\n",
    "        #Prepare the encoder and decoder for VAE.\n",
    "        if(enc_chk!= \"\"):\n",
    "            self.encoder = RobertaForSequenceClassification.from_pretrained(check_point).to(device)\n",
    "            self.encoder.load_state_dict(torch.load(enc_chk))\n",
    "            self.encoder = self.encoder.roberta\n",
    "        else:\n",
    "            self.encoder = RobertaModel.from_pretrained(check_point).to(device)\n",
    "        for param in self.encoder.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(check_point)\n",
    "        self.config = AutoConfig.from_pretrained(check_point)\n",
    "\n",
    "        self.cmi = torch.nn.Parameter(torch.rand(10, requires_grad=True, device=device))\n",
    "        self.dmi = torch.nn.Parameter(torch.rand(10, requires_grad=True, device=device))\n",
    "\n",
    "        hidden_size = self.config.hidden_size\n",
    "        if decoder_type == 'BART':\n",
    "            self.decoder = BartDecoder.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder = BartForSequenceClassification.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder.load_state_dict(torch.load(dec_chk))\n",
    "            # self.decoder = self.decoder.model.decoder\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(hidden_size, hidden_size, 1, batch_first=True)\n",
    "\n",
    "        self.decoder_start_token_id = 2\n",
    "        self.lm_head = nn.Linear(hidden_size, self.config.vocab_size)\n",
    "        # self.target_labeler = FCLayer(hidden_size,num_targets,0.2)\n",
    "        self.lm_loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.x_sigma = x_sigma\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gamma = nn.Parameter(torch.tensor(gamma, requires_grad=True, device=device))\n",
    "        self.eta = nn.Parameter(torch.tensor(eta, requires_grad=True, device=device))\n",
    "        self.compressor = nn.Linear(2*hidden_size,hidden_size)\n",
    "\n",
    "        self.dropout_rate = params1.classifier_dropout\n",
    "\n",
    "        #Prepare the disentanglement modules.\n",
    "        self.continuous_inference = nn.Sequential()\n",
    "        #self.disc_latent_inference = nn.Sequential()\n",
    "        conti_mean_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                           latent_dim=hidden_size,\n",
    "                                           disc_variable=False)\n",
    "        conti_logsigma_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                               latent_dim=hidden_size,\n",
    "                                               disc_variable=False)\n",
    "        self.continuous_inference.add_module(\"mean\", conti_mean_inf_module)\n",
    "        self.continuous_inference.add_module(\"log_sigma\", conti_logsigma_inf_module)\n",
    "\n",
    "        # conti_mean_inf_module_t = _Inference(num_input_channels=hidden_size,\n",
    "        #                                    latent_dim=hidden_size,\n",
    "        #                                    disc_variable=False)\n",
    "        # conti_logsigma_inf_module_t = _Inference(num_input_channels=hidden_size,\n",
    "        #                                        latent_dim=hidden_size,\n",
    "        #                                        disc_variable=False)\n",
    "        # self.disc_latent_inference.add_module(\"mean\", conti_mean_inf_module_t)\n",
    "        # self.disc_latent_inference.add_module(\"log_sigma\", conti_logsigma_inf_module_t)\n",
    "        self._disc_latent_dim = num_targets\n",
    "        # dic_inf = _Inference(num_input_channels=hidden_size, latent_dim=self._disc_latent_dim,\n",
    "        #                      disc_variable=True)\n",
    "        # self.disc_latent_inference = dic_inf\n",
    "        sample = Sample(temperature=self.temperature)\n",
    "        self.sample = sample\n",
    "\n",
    "        self.kl_beta_c = beta_c\n",
    "        self.kl_beta_d = beta_d\n",
    "\n",
    "        self.disc_log_prior_param = torch.log(torch.tensor([1 / self._disc_latent_dim for i in range(self._disc_latent_dim)]).view(1, -1).float().cuda())\n",
    "\n",
    "\n",
    "        #Reconstructor\n",
    "        self.reconstructor = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = hidden_size)\n",
    "        #Hate Classifier\n",
    "        self.hate_classifier = nn.Sequential(\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size, num_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        #Target Mapper\n",
    "        self.target_mapper = nn.Sequential(\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        #Target Classifier\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Tanh(),\n",
    "            # nn.Dropout(params1.classifier_dropout),\n",
    "            # nn.Linear(hidden_size, num_targets)\n",
    "        )\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def compute_sample_weight(self,y_pred):\n",
    "      y_pred = y_pred.float()\n",
    "      C = y_pred.shape[1]\n",
    "\n",
    "      y_pred_clamped = torch.clamp(y_pred, 1e-9, 1.0)\n",
    "      \n",
    "      entropy = -torch.sum(y_pred_clamped * torch.log(y_pred_clamped), dim=1)\n",
    "      max_entropy = torch.log(torch.tensor(C).float())\n",
    "      weight = 1 - (entropy / max_entropy)\n",
    "      \n",
    "      return weight\n",
    "\n",
    "    def get_contrastive_loss(self, embeddings, labels):\n",
    "      sf = nn.Softmax(dim=1)\n",
    "      softmax_labels = sf(labels)\n",
    "      weights = self.compute_sample_weight(softmax_labels)\n",
    "      max_indices = torch.argmax(softmax_labels, dim=1)\n",
    "      weights = (max_indices.unsqueeze(1) == max_indices.unsqueeze(0)).float()\n",
    "\n",
    "      tensor_expanded = embeddings.unsqueeze(1) - embeddings.unsqueeze(0)\n",
    "      distance_matrix = torch.sqrt(torch.sum(tensor_expanded ** 2, dim=-1) + 1e-8) / (embeddings.shape[1] + 1e-8)\n",
    "      mask = weights > self.eta\n",
    "      contrastive_loss = weights * mask * distance_matrix.pow(2) + (1 - weights) * mask * F.relu(self.gamma - distance_matrix).pow(2)\n",
    "      # Sum over all dimensions and average over the batch\n",
    "      loss = torch.sum(contrastive_loss) / (mask.sum() + 1e-8)\n",
    "      return loss,weights\n",
    "      # loss = torch.mean(weights * distance_matrix.pow(2) + (1 - weights) * torch.clamp_min(self.gamma - distance_matrix, 0).pow(2), axis=1)\n",
    "      # return torch.sum(loss)\n",
    "\n",
    "\n",
    "    def get_lm_loss(self, logits, labels, masks):\n",
    "        '''Get the utterance reconstruction loss.'''\n",
    "        # labels = labels.float()\n",
    "        loss = self.lm_loss_fn(logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        masked_loss = loss * masks.view(-1)\n",
    "        return torch.mean(masked_loss)\n",
    "    \n",
    "    def confidence_regularization(self,predictions):\n",
    "      C = predictions.shape[1]\n",
    "      uniform_distribution = torch.full_like(predictions, 1.0 / C)\n",
    "      \n",
    "      # Compute the KL divergence loss\n",
    "      kl_div = F.kl_div(predictions.log(), uniform_distribution, reduction='batchmean')\n",
    "      \n",
    "      return kl_div\n",
    "\n",
    "\n",
    "    def forward(self, inputs, mask, decoder_inputs, decoder_masks, decoder_labels, vad_labels, labels):\n",
    "        x = self.encoder(inputs, attention_mask=mask)[0]\n",
    "        x = x[:, 0, :].squeeze(1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-6)\n",
    "\n",
    "        if torch.isnan(x).any():\n",
    "          print(\"NaN values detected in BatchNorm output\")\n",
    "\n",
    "        #Get the latent variables.\n",
    "        norm_mean = self.continuous_inference.mean(x)\n",
    "        norm_log_sigma = self.continuous_inference.log_sigma(x)\n",
    "        # norm_mean_t = self.disc_latent_inference.mean(x)\n",
    "        # norm_log_sigma_t = self.disc_latent_inference.log_sigma(x)\n",
    "        t_sample = self.target_mapper(x)\n",
    "        latent_sample = self.sample(norm_mean, norm_log_sigma,t_sample)\n",
    "        t_logits = self.target_classifier(t_sample)\n",
    "        #target_weak_labels = self.softmax(t_logits)\n",
    "        # print(t_logits)\n",
    "        con_loss, weights = self.get_contrastive_loss(t_sample,t_logits)\n",
    "\n",
    "        # print(con_loss)\n",
    "\n",
    "        latent_sample = torch.squeeze(latent_sample)\n",
    "        latent_sample = self.compressor(latent_sample)\n",
    "        decoder_hidden = self.reconstructor(latent_sample)\n",
    "\n",
    "        if self.decoder_type == 'BART':\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_inputs,\n",
    "                attention_mask=decoder_masks,\n",
    "                encoder_hidden_states=decoder_hidden.unsqueeze(1))\n",
    "            lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        else:\n",
    "            input_embeddings = self.encoder.embeddings(decoder_inputs)\n",
    "            h = decoder_hidden.unsqueeze(0)\n",
    "            decoder_outputs, (_, _) = self.decoder(input_embeddings, (h, torch.zeros(h.shape).to(self.device)))\n",
    "            lm_logits = self.lm_head(decoder_outputs)\n",
    "\n",
    "        reconstruct_loss = self.get_lm_loss(lm_logits, decoder_labels, decoder_masks)#/ (2 * self.batch_size * (self.x_sigma ** 2))\n",
    "\n",
    "\n",
    "        # calculate latent space KL divergence\n",
    "        z_mean_sq = norm_mean * norm_mean\n",
    "        z_log_sigma_sq = 2 * norm_log_sigma\n",
    "        z_sigma_sq = torch.exp(z_log_sigma_sq)\n",
    "        continuous_kl_loss = 0.5 * torch.sum(z_mean_sq + z_sigma_sq - z_log_sigma_sq - 1) / self.batch_size\n",
    "        # notice here we duplicate the 0.5 by each part\n",
    "        # disc param : log(a1),...,log(an) type\n",
    "        mask = weights > self.eta\n",
    "        log_q = torch.log(torch.tensor(1.0 / self._disc_latent_dim)).cuda()\n",
    "        kl_div = torch.sum(torch.exp(t_logits) * (t_logits - log_q), dim=1)  # Sum over the dimension representing different classes\n",
    "        weighted_kl_div = weights * mask * kl_div\n",
    "        disc_kl_loss = torch.mean(weighted_kl_div)\n",
    "        #disc_kl_loss = torch.mean(weights * mask * torch.sum(torch.exp(t_logits) * (t_logits - self.disc_log_prior_param)) / self.batch_size)\n",
    "\n",
    "        prior_kl_loss_l = self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi) + self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi)\n",
    "        elbo_loss_l = reconstruct_loss + prior_kl_loss_l\n",
    "\n",
    "        confidence_loss = self.confidence_regularization(self.softmax(t_logits))\n",
    "\n",
    "        #Calculate the classification loss.\n",
    "        hate_sample = self.sample._sample_norm(norm_mean, norm_log_sigma)\n",
    "        #target_sample = self.sample._sample_gumbel_softmax(disc_log_alpha)\n",
    "\n",
    "        # hate_sample = F.dropout(hate_sample, p=self.dropout_rate, training=self.training)\n",
    "        # target_sample = self.get_vad_loss(target_sample,vad_labels)\n",
    "        hate_logits = self.hate_classifier(hate_sample)\n",
    "\n",
    "        # target_labels = torch.zeros_like(vad_labels).float().cuda()\n",
    "        # target_labels[target_logits] = 1\n",
    "\n",
    "        # print(\"Reconstruction loss\",reconstruct_loss,\"KL Continous Loss\",self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi),\"KL Discrete loss\",self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi),end=\",\")\n",
    "        return elbo_loss_l[0], hate_logits, con_loss,confidence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hate_model,train_data, train_labels, train_target, val_data, val_labels, val_target, encoder_tokenizer, decoder_tokenizer, params1):\n",
    "    accumulation_steps = 16\n",
    "    train = EncodedDataset(input_sents=train_data,\n",
    "                    input_labels=train_labels,\n",
    "                    target_labels=train_target,\n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer,\n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "\n",
    "    val =  EncodedDataset(input_sents=val_data,\n",
    "                    input_labels=val_labels,\n",
    "                    target_labels=val_target,\n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer,\n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "\n",
    "\n",
    "    sampler = WeightedRandomSampler(params1.h_weights, len(params1.h_weights))\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=params1.train_batch_size,drop_last=True,sampler=sampler)\n",
    "    val_dataloader = DataLoader(val, batch_size=params1.val_batch_size,drop_last=True)\n",
    "\n",
    "    vae_optimizer = torch.optim.AdamW(hate_model.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    # hate_optimizer = torch.optim.AdamW(hate_model.hate_classifier.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    # target_optimizer = torch.optim.AdamW(hate_model.target_classifier.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    hate_loss = nn.CrossEntropyLoss(params1.hate_class_weights)\n",
    "    #target_loss = nn.CrossEntropyLoss(params1.target_class_weights)\n",
    "\n",
    "    s_total_steps = float(10 * len(train)) / params1.train_batch_size\n",
    "    v_scheduler = get_linear_schedule_with_warmup(vae_optimizer, int(s_total_steps * params1.warmup_ratio),math.ceil(s_total_steps))\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.9,patience=2)\n",
    "\n",
    "    # hate_opt = torch.optim.Adam(hate_model.parameters(), lr=params1.content_lr, weight_decay = 1e-2)\n",
    "    save_dir = \"D:/Hate Speech/Models/Disentanglement/WeakSupervision/\"\n",
    "\n",
    "    best_validation_accuracy = 1e-5\n",
    "    print(\"Training started!\")\n",
    "\n",
    "    e=0\n",
    "    for epoch in range(params1.num_epochs):\n",
    "        total_vae_loss = 0\n",
    "        total_adversary_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_acc_train = 0\n",
    "        total_acc_target = 0\n",
    "        predictions = []\n",
    "        y_true = []\n",
    "        loss_list = []\n",
    "        predicts = []\n",
    "        ground_truth = []\n",
    "        c=0\n",
    "        cnt=0\n",
    "        hate_model.train()\n",
    "        for train_input, train_mask, train_target, train_label,train_dec_input, train_dec_mask in train_dataloader:\n",
    "            hate_model.zero_grad()\n",
    "            c+=1\n",
    "            cnt+=1\n",
    "            train_input = train_input.to(device)\n",
    "            train_mask = train_mask.to(device)\n",
    "            train_target = train_target.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "            train_dec_input = train_dec_input.to(device)\n",
    "            train_dec_mask = train_dec_mask.to(device)\n",
    "            elbo_loss_l, hate_logits, con_loss, confidence_loss = hate_model(train_input, train_mask, train_dec_input, train_dec_mask, train_input,train_target, train_label)\n",
    "            h_loss = hate_loss(hate_logits, train_label)\n",
    "            #vad_loss = target_loss(target_logits,torch.argmax(train_target,dim=1))\n",
    "            # vad_loss = target_loss(target_logits, train_target)\n",
    "            # # Regularization losses\n",
    "            # l2_strength = 1e-4\n",
    "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
    "            # for name, param in hate_model.named_parameters():\n",
    "            #     if 'weight' in name:\n",
    "            #         l2_loss = l2_loss + torch.norm(param, p=2)\n",
    "            # loss = h_loss + params1.alpha*elbo_loss_l + l2_strength * l2_loss + params1.target_loss_coeff*vad_loss\n",
    "            loss = params1.hate_coeff*h_loss + params1.alpha*elbo_loss_l + params1.target_loss_coeff*con_loss + params1.target_loss_coeff*confidence_loss\n",
    "            # loss = vad_loss\n",
    "            if((c+1)%accumulation_steps==0):\n",
    "                loss.backward()\n",
    "                # for name, param in hate_model.named_parameters():\n",
    "                #   if param.grad is not None:\n",
    "                #       print(f\"Layer: {name}, Gradient norm: {param.grad.data.norm(2)}\")\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(hate_model.parameters(), 1.0)\n",
    "                vae_optimizer.step()\n",
    "                # hate_optimizer.step()\n",
    "                # target_optimizer.step()\n",
    "                v_scheduler.step()\n",
    "                # h_scheduler.step()\n",
    "                # t_scheduler.step()\n",
    "\n",
    "            if(c%100==0):\n",
    "                print(\"Temperature\",hate_model.temperature.item(),\"batch\",c,\"epoch\",epoch,\"1s\",len(np.where(torch.argmax(hate_logits, dim=1).cpu().numpy()==1)[0]),\"loss\",loss.item(),\"h_loss\",h_loss.item(),\"elbo_loss\",params1.alpha*elbo_loss_l.item(),\"vad_loss\",params1.target_loss_coeff*con_loss.item())\n",
    "            ground_truth += train_label.cpu().numpy().tolist()\n",
    "            predicts += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "            loss_list.append(loss.item())\n",
    "            acc = round(accuracy_score(torch.argmax(train_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "            #target_acc = round(accuracy_score(torch.argmax(train_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "            total_vae_loss += elbo_loss_l.item()\n",
    "            #total_adversary_loss += vad_loss.item()\n",
    "            total_cls_loss += h_loss.item()\n",
    "            total_acc_train += acc\n",
    "            #total_acc_target += target_acc\n",
    "            # print(\"PREDS: \",torch.argmax(target_logits, dim=1).cpu().numpy().tolist())\n",
    "            # print(\"TARGETS: \",torch.argmax(train_target, dim=1).cpu().numpy().tolist())\n",
    "            if(c%100==0):\n",
    "                print(\"Train Accuracy\",acc)# Train Target Accuracy\",target_acc)\n",
    "            # if(c==500):\n",
    "            #     break\n",
    "        y_true = []\n",
    "        predictions = []\n",
    "        hate_model.eval()\n",
    "        total_loss_val = 0\n",
    "        total_acc_val = 0\n",
    "        total_acc_target_val = 0\n",
    "        e_cnt=0\n",
    "        print(\"VALIDATION\")\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_mask, val_target, val_label, val_dec_input, val_dec_mask in val_dataloader:\n",
    "                e_cnt+=1\n",
    "                val_input = val_input.to(device)\n",
    "                val_mask = val_mask.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "                val_label = val_label.to(device)\n",
    "                val_dec_input = val_dec_input.to(device)\n",
    "                val_dec_mask = val_dec_mask.to(device)\n",
    "                elbo_loss_l, hate_logits, target_logits, loss2 = hate_model(val_input, val_mask, val_dec_input, val_dec_mask, val_input,val_target, val_label)\n",
    "                total_acc_val += round(accuracy_score(torch.argmax(val_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "                #total_acc_target_val += round(accuracy_score(torch.argmax(val_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits,dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "                y_true += torch.argmax(val_label, dim=1).cpu().numpy().tolist()\n",
    "                predictions += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "            print(classification_report(y_true, predictions))\n",
    "\n",
    "        metrics = {\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train ELBO Loss\" : total_vae_loss/cnt,\n",
    "            \"Train Target Loss\" : total_adversary_loss/cnt,\n",
    "            \"Train Hate Loss\" : total_cls_loss/cnt,\n",
    "            \"Train Hate Accuracy\" : total_acc_train/cnt,\n",
    "            \"Train Target Accuracy\" : total_acc_target/cnt,\n",
    "            \"Validation Loss\" : total_loss_val/e_cnt,\n",
    "            \"Validation Hate Accuracy\" : total_acc_val/e_cnt,\n",
    "            \"Validation Target Accuracy\" : total_acc_target_val/e_cnt}\n",
    "\n",
    "        print(metrics)\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        val_metrics = {\"F1-Score\": classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score']}\n",
    "        wandb.log({**metrics, **val_metrics})\n",
    "\n",
    "        if(best_validation_accuracy <= round(classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score'],3)):\n",
    "            best_validation_accuracy = round(classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score'],3)\n",
    "            best_report = classification_report(y_true, predictions)\n",
    "            e=0\n",
    "            print(\"E from if = \",e)\n",
    "            # fname = \"best-model_\" + params1.dataset_name+\"_\"+str(epoch+1)+\"_VAE_with_rob_base_NoFT_balancedData.pt\"\n",
    "            fname = \"best-model_\" + params1.dataset_name+\"_\"+str(epoch+1)+params1.fname\n",
    "            torch.save(hate_model.state_dict(), os.path.join(save_dir, fname))\n",
    "            print(\"Saved at \",os.path.join(save_dir, fname))\n",
    "        else:\n",
    "            print(\"E = \",e)\n",
    "            e+=1\n",
    "        if(e==3):\n",
    "            print(best_report)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# files = ['D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv',\n",
    "#            'D:/Hate Speech/Preprocessed Datasets/Reddit/reddit_TRY_Numeric_train.csv',\n",
    "#            'D:/Hate Speech/Preprocessed Datasets/Twitter/twitter_HX_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_train.csv',\n",
    "#             'D:/Hate Speech/Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced.csv']\n",
    "# files = [\"D:/Hate Speech/Preprocessed Datasets/RedditNew/RedditNew_Train.csv\",\n",
    "#               \"D:/Hate Speech/Preprocessed Datasets/TwitterNew/TwitterNew_Train.csv\"]\n",
    "\n",
    "files = ['D:/Hate Speech/Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced.csv']\n",
    "\n",
    "test_files = [\"D:/Hate Speech/Preprocessed Datasets/RedditNew/RedditNew_Test.csv\",\n",
    "              \"D:/Hate Speech/Preprocessed Datasets/TwitterNew/TwitterNew_Test.csv\"]#'D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "        #  'D:/Hate Speech/Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "        #   'D:/Hate Speech/Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv',\n",
    "        #    'D:/Hate Speech/Preprocessed Datasets/Youtube/youtube_IC_Numeric_test.csv']\n",
    "\n",
    "enc_files = [#'D:/Hate Speech/Finetuned/Roberta/gab_HX/pytorch_model.bin',\n",
    "            'D:/Hate Speech/Encoder_FT/Roberta/RedditNew/pytorch_model.bin',\n",
    "            'D:/Hate Speech/Encoder_FT/Roberta/TwitterNew/pytorch_model.bin']\n",
    "            # './FineTuned/Roberta/twitter_TRY/pytorch_model.bin',\n",
    "            # './FineTuned/Roberta/twitter/pytorch_model.bin',\n",
    "            # './FineTuned/Roberta/youtube/pytorch_model.bin',\n",
    "            # './FineTuned/Roberta/youtube_TRY/pytorch_model.bin',\n",
    "            #'D:/Hate Speech/Finetuned/Roberta/youtube_IC/pytorch_model.bin']\n",
    "\n",
    "# enc_files = ['./FineTuned/Roberta/gab/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/reddit_TRY/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/twitter/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/youtube/pytorch_model.bin']\n",
    "\n",
    "dec_files = [#'D:/Hate Speech/Finetuned/BART/pytorch_model.bin',\n",
    "             'D:/Hate Speech/Decoder_FT/BART/RedditNew/pytorch_model.bin',\n",
    "             'D:/Hate Speech/Decoder_FT/BART/TwitterNew/pytorch_model.bin']\n",
    "             #'D:/Hate Speech/Finetuned/BART_youtube/pytorch_model.bin']\n",
    "\n",
    "\n",
    "dataset_names = [\"Reddit_New\",\"Twitter_New\"]#[\"gab_HX\",\"reddit_TRY\",\"twitter_HX\",\"youtube_IC\"]#\"twitter_TRY\",\"twitter\",\"youtube\",\"youtube_TRY\",\n",
    "hidden_size = 768\n",
    "classifier_dropout = 0.2\n",
    "learning_rate = 5e-3\n",
    "print(files)\n",
    "print(len(files))\n",
    "print(device)\n",
    "filenames = set()\n",
    "latent_variables = ['hate','target']#['0','1','2','3','4','5','6','7']\n",
    "\n",
    "for f in range(0,len(files)):\n",
    "        torch.cuda.empty_cache()\n",
    "        train_frame = pd.read_csv(files[f])\n",
    "        tar = np.zeros(8)\n",
    "        #a1 = np.unique(train_frame['target'])\n",
    "        #class_weights = compute_class_weight('balanced', classes=a1, y=train_frame['target'])\n",
    "        # for i in range(len(a1)):\n",
    "        #     try:\n",
    "        #         tar[a1[i]] = class_weights[a1[i]]\n",
    "        #     except:\n",
    "        #         tar[a1[i]] = 0\n",
    "        # print(tar)\n",
    "        # class_weights2 = torch.FloatTensor(tar).to(device)\n",
    "        #print(class_weights2)\n",
    "        if(dataset_names[f] not in filenames):\n",
    "            filenames.add(dataset_names[f])\n",
    "            # train_frame = pd.read_csv(files[f])\n",
    "            if(f+1<len(files)):\n",
    "                print(\"TEST FILE: \",test_files[f+1])\n",
    "                test_frame = pd.read_csv(test_files[f+1])\n",
    "            else:\n",
    "                 test_frame = pd.read_csv(test_files[0])\n",
    "            class_weights1 = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
    "            class_weights1 = torch.FloatTensor(class_weights1)\n",
    "            x = train_frame['label'].value_counts().values\n",
    "            class_weights3 = torch.FloatTensor([x[0]/sum(x),x[1]/sum(x)])\n",
    "            # class_weights3 = torch.FloatTensor([0.6,0.4])\n",
    "            h_weights = class_weights3[train_frame['label']]\n",
    "            print(h_weights)\n",
    "            class_weights1 = class_weights1.to(device)\n",
    "            wandb.init(\n",
    "                # Set the project where this run will be logged\n",
    "                project=\"Disentangling Hate Speech and Target\", \n",
    "                # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "                name=f\"experiment_ROB_BART_{dataset_names[f]}_{classifier_dropout}_{learning_rate}\",\n",
    "                # Track hyperparameters and run metadata\n",
    "                config = { \n",
    "                        \"max_sequence_length\": 512, \n",
    "                        \"train_batch_size\" : 8, \n",
    "                        \"val_batch_size\" : 8,\n",
    "                        \"hidden_dim\" : 512, \n",
    "                        \"hate_dim\" : 384,\n",
    "                        \"num_epochs\" : 100, \n",
    "                        \"device\" : device,\n",
    "                        \"dataset_name\" : dataset_names[f],\n",
    "                        \"h_weights\" : h_weights,\n",
    "                        \"hate_class_weights\" : class_weights1,\n",
    "                        #\"target_class_weights\" : class_weights2,\n",
    "                        \"hidden_size\" : 768,\n",
    "                        \"num_labels\": 2,\n",
    "                        \"num_targets\": 8,\n",
    "                        \"classifier_dropout\" : classifier_dropout,\n",
    "                        \"content_lr\": 2e-5,\n",
    "                        \"decoder_type\" : \"BART\",\n",
    "                        \"kl_weight\" : 0.05,\n",
    "                        \"mi_loss_weight\" : 0.001,\n",
    "                        \"mi_loss\" : False,\n",
    "                        \"alpha\" : 1,\n",
    "                        \"beta_c\" : 0.05,\n",
    "                        \"beta_d\" : 0.05,\n",
    "                        \"warmup_ratio\" : 0.2,\n",
    "                        \"weight_decay\" : 0.001,\n",
    "                        \"target_loss_coeff\" : 1,\n",
    "                        \"hate_coeff\" : 1,\n",
    "                        \"fname\" : \"_VAE_with_rob_base_WS_OURS.pt\",\n",
    "            })\n",
    "            params1 = { \n",
    "                        \"max_sequence_length\": 512, \n",
    "                        \"train_batch_size\" : 8, \n",
    "                        \"val_batch_size\" : 8,\n",
    "                        \"hidden_dim\" : 512, \n",
    "                        \"hate_dim\" : 384,\n",
    "                        \"num_epochs\" : 100, \n",
    "                        \"device\" : device,\n",
    "                        \"dataset_name\" : dataset_names[f],\n",
    "                        \"h_weights\" : h_weights,\n",
    "                        \"hate_class_weights\" : class_weights1,\n",
    "                        #\"target_class_weights\" : class_weights2,\n",
    "                        \"hidden_size\" : 768,\n",
    "                        \"num_labels\": 2,\n",
    "                        \"num_targets\": 8,\n",
    "                        \"classifier_dropout\" : classifier_dropout,\n",
    "                        \"content_lr\": 2e-5,\n",
    "                        \"decoder_type\" : \"BART\",\n",
    "                        \"kl_weight\" : 0.05,\n",
    "                        \"mi_loss_weight\" : 0.001,\n",
    "                        \"mi_loss\" : False,\n",
    "                        \"alpha\" : 1,\n",
    "                        \"beta_c\" : 0.05,\n",
    "                        \"beta_d\" : 0.05,\n",
    "                        \"warmup_ratio\" : 0.2,\n",
    "                        \"weight_decay\" : 0.001,\n",
    "                        \"target_loss_coeff\" : 1,\n",
    "                        \"hate_coeff\" : 1,\n",
    "                        \"fname\" : \"_VAE_with_rob_base_WS_OURS.pt\",\n",
    "            }\n",
    "            params1 = SimpleNamespace(**params1)\n",
    "            # enc_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "            enc_model_name = \"roberta-base\"\n",
    "            dec_model_name = \"facebook/bart-base\"\n",
    "            # enc_chk = \"./FineTuned/Roberta/pytorch_model.bin\"\n",
    "            print(enc_files[f])\n",
    "            print(params1.fname)\n",
    "            enc_chk = enc_files[f]#\"./FineTuned/RobertaHate/pytorch_model.bin\"\n",
    "            dec_chk = dec_files[f]#\"./FineTuned/BART/pytorch_model.bin\"\n",
    "            hate_model = HATE_WATCH(1.0,0.1,0.95,enc_chk,enc_model_name,dec_chk,dec_model_name, params1.num_targets,params1.num_labels,params1.beta_c, params1.beta_d,device,params1.train_batch_size,params1.decoder_type).to(device)\n",
    "            encoder_tokenizer = RobertaTokenizer.from_pretrained(enc_model_name)\n",
    "            decoder_tokenizer = BartTokenizer.from_pretrained(dec_model_name)\n",
    "            print(params1.dataset_name)\n",
    "            print(files[f])\n",
    "            print(train_frame.shape)\n",
    "            wandb.watch(hate_model)\n",
    "            train(hate_model=hate_model,\n",
    "            train_data=train_frame['text'].values.tolist(), \n",
    "            train_labels=train_frame['label'].values.tolist(), \n",
    "            train_target=[\"\"]*len(train_frame['label'].values.tolist()), \n",
    "            val_data=test_frame['text'].values.tolist(), \n",
    "            val_labels=test_frame['label'].values.tolist(), \n",
    "            val_target = [\"\"]*len(train_frame['label'].values.tolist()),\n",
    "            encoder_tokenizer=encoder_tokenizer,\n",
    "            decoder_tokenizer=decoder_tokenizer,\n",
    "            params1=params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, test_labels, target_labels, encoder_tokenizer, decoder_tokenizer, params1):\n",
    "    \n",
    "    test = EncodedDataset(input_sents=test_data, \n",
    "                    input_labels=test_labels, \n",
    "                    target_labels=target_labels,  \n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer,\n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "    \n",
    "\n",
    "    val_dataloader = DataLoader(test, batch_size=params1.val_batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    total_acc_val = 0\n",
    "    total_acc_target_val = 0\n",
    "    hate_model.eval()\n",
    "    e_cnt=0\n",
    "    with torch.no_grad():\n",
    "      for val_input, val_mask, val_target, val_label, val_dec_input, val_dec_mask in val_dataloader:\n",
    "        e_cnt+=1\n",
    "        val_input = val_input.to(device)\n",
    "        val_mask = val_mask.to(device)\n",
    "        val_target = val_target.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "        val_dec_input = val_dec_input.to(device)\n",
    "        val_dec_mask = val_dec_mask.to(device)\n",
    "        elbo_loss_l, hate_logits, target_logits, _ = hate_model(val_input, val_mask, val_dec_input, val_dec_mask, val_input,val_target, val_label)\n",
    "        total_acc_val += round(accuracy_score(torch.argmax(val_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "        #total_acc_target_val += round(accuracy_score(torch.argmax(val_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits,dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "        y_true += torch.argmax(val_label, dim=1).cpu().numpy().tolist()\n",
    "        predictions += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "      print(classification_report(y_true, predictions))\n",
    "      print(\"Accuracy: \", total_acc_val/e_cnt)\n",
    "      #print(\"Target Accuracy: \", total_acc_target_val/e_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_files = [#'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_1_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_2_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_3_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_4_VAE_with_rob_base_ft.pt',\n",
    "#                './BuildingFramework/Models/Disentanglement/best-model_gab_HX_13_VAE_with_rob_base_ft_balancedData.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_19_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "#                './BuildingFramework/Models/Disentanglement/best-model_gab_HX_6_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "#                './BuildingFramework/Models/Disentanglement/best-model_gab_HX_6_VAE_with_rob_base_NoBL_balancedData.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_ft.pt',\n",
    "#                './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_2_VAE_with_rob_base_ft_balancedData.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_3_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "#                './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "#                './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_3_VAE_with_rob_base_NoTL_balancedData.pt',\n",
    "#                './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_NoBL_balancedData.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_1_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_2_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_3_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_4_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_5_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_6_VAE_with_rob_base_ft.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_6_VAE_with_rob_base_ft_balancedData.pt',\n",
    "#               # './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_7_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_3_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_11_VAE_with_rob_base_NoTL_balancedData.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_3_VAE_with_rob_base_NoBL_balancedData.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_TRY_1_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_TRY_2_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_twitter_1_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_twitter_2_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_1_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_2_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_TRY_1_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_1_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_4_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_5_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_6_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_8_VAE_with_rob_base_ft.pt',\n",
    "#                #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_11_VAE_with_rob_base_ft.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_10_VAE_with_rob_base_ft_balancedData.pt',\n",
    "#               # './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_1_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_2_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_7_VAE_with_rob_base_NoTL_balancedData.pt',\n",
    "#                 './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_2_VAE_with_rob_base_NoBL_balancedData.pt']\n",
    "\n",
    "# model_files = [\"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_gab_HX_24_VAE_with_rob_base_WS_OURS.pt\",\n",
    "#                \"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_reddit_TRY_6_VAE_with_rob_base_WS_OURS.pt\",\n",
    "#                \"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_twitter_HX_6_VAE_with_rob_base_WS_OURS.pt\",\n",
    "#                \"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_youtube_IC_10_VAE_with_rob_base_WS_OURS.pt\"]\n",
    "\n",
    "model_files = [\"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_Twitter_New_1_VAE_with_rob_base_WS_OURS.pt\",\n",
    "                \"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_Reddit_New_4_VAE_with_rob_base_WS_OURS.pt\"]\n",
    "\n",
    "test_files =['D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "          'D:/Hate Speech/Preprocessed Datasets/RedditNew/RedditNew_Test.csv',\n",
    "           'D:/Hate Speech/Preprocessed Datasets/TwitterNew/TwitterNew_Test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "             'D:/Hate Speech/Preprocessed Datasets/Youtube/youtube_Numeric_test.csv']\n",
    "        #     'D:/Hate Speech/Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv']\n",
    "        #    'D:/Hate Speech/Preprocessed Datasets/Youtube/youtube_IC_Numeric_test.csv']\n",
    "# test_files = ['./reddit_TRY_Numeric_test_subset.csv']\n",
    "\n",
    "for f in model_files:\n",
    "        print(f)\n",
    "        print(\"*\"*100)\n",
    "        for f1 in range(0,len(test_files)):\n",
    "                print(test_files[f1])\n",
    "                print(\"*\"*100)\n",
    "                hate_model.load_state_dict(torch.load(f))\n",
    "                test_frame =  pd.read_csv(test_files[f1])\n",
    "                evaluate(model=hate_model, test_data = test_frame['text'].values.tolist(), test_labels=test_frame['label'].values.tolist(),target_labels=[\"\"]*len(test_frame['label'].values.tolist()), encoder_tokenizer=encoder_tokenizer,decoder_tokenizer=decoder_tokenizer,params1=params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_dict = {\"Race\":0,\"Religion\":1,\"Sexuality and Sexual Preferences\":2,\"Gender\":3,\"Immigration Status\":4,\"Nationality\":5,\"Ableness/Disability\":6,\"Class\":7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Inference(nn.Sequential):\n",
    "    def __init__(self, num_input_channels, latent_dim, disc_variable=True):\n",
    "        super(_Inference, self).__init__()\n",
    "        if disc_variable:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, num_input_channels//2))\n",
    "            self.add_module('relu', nn.ReLU())\n",
    "            self.add_module('fc2', nn.Linear(num_input_channels//2, latent_dim))\n",
    "            self.add_module('log_softmax', nn.LogSoftmax(dim=1))\n",
    "        else:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, latent_dim))\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(Sample, self).__init__()\n",
    "        self._temperature = temperature\n",
    "\n",
    "    def forward(self, norm_mean, norm_log_sigma, norm_mean_t, norm_log_sigma_t, disc_label=None, mixup=False, disc_label_mixup=None,\n",
    "                mixup_lam=None):\n",
    "        \"\"\"\n",
    "        :param norm_mean: mean parameter of continuous norm variable\n",
    "        :param norm_log_sigma: log sigma parameter of continuous norm variable\n",
    "        :param disc_log_alpha: log alpha parameter of discrete multinomial variable\n",
    "        :param disc_label: the ground truth label of discrete variable (not one-hot label)\n",
    "        :param mixup: if we do mixup\n",
    "        :param disc_label_mixup: the mixup target label\n",
    "        :param mixup_lam: the mixup lambda\n",
    "        :return: sampled latent variable\n",
    "        \"\"\"\n",
    "        batch_size = norm_mean.size(0)\n",
    "        latent_sample = list([])\n",
    "        latent_sample.append(self._sample_norm(norm_mean, norm_log_sigma))\n",
    "        latent_sample.append(self._sample_norm(norm_mean_t, norm_log_sigma_t))\n",
    "        latent_sample = torch.cat(latent_sample, dim=1)\n",
    "        dim_size = latent_sample.size(1)\n",
    "        latent_sample = latent_sample.view(batch_size, dim_size, 1, 1)\n",
    "        return latent_sample\n",
    "\n",
    "    def _sample_gumbel_softmax(self, log_alpha):\n",
    "        \"\"\"\n",
    "        Samples from a gumbel-softmax distribution using the reparameterization\n",
    "        trick.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_alpha : torch.Tensor\n",
    "            Parameters of the gumbel-softmax distribution. Shape (N, D)\n",
    "        \"\"\"\n",
    "        EPS = 1e-12\n",
    "        unif = torch.rand(log_alpha.size()).cuda()\n",
    "        gumbel = -torch.log(-torch.log(unif + EPS) + EPS)\n",
    "        # Reparameterize to create gumbel softmax sample\n",
    "        logit = (log_alpha + gumbel) / self._temperature\n",
    "        return torch.softmax(logit, dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_norm(mu, log_sigma):\n",
    "        \"\"\"\n",
    "        :param mu: the mu for sampling with N*D\n",
    "        :param log_sigma: the log_sigma for sampling with N*D\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        std_z = torch.randn(mu.size())\n",
    "        if mu.is_cuda:\n",
    "            std_z = std_z.cuda()\n",
    "\n",
    "        return mu + torch.exp(log_sigma) * std_z\n",
    "\n",
    "class HATE_WATCH(nn.Module):\n",
    "    \"\"\"The VAD-VAE model.\"\"\"\n",
    "    def __init__(self, temperature,gamma,enc_chk,check_point, dec_chk,bart_check_point,num_targets, num_class, beta_c,beta_d, device, batch_size, decoder_type, x_sigma=1):\n",
    "        super(HATE_WATCH, self).__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_type = decoder_type\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature, requires_grad=True, device=device))\n",
    "\n",
    "        #Prepare the encoder and decoder for VAE.\n",
    "        # self.encoder = RobertaForSequenceClassification.from_pretrained(check_point).to(device)\n",
    "        if(enc_chk!= \"\"):\n",
    "            self.encoder = RobertaForSequenceClassification.from_pretrained(check_point).to(device)\n",
    "            self.encoder.load_state_dict(torch.load(enc_chk))\n",
    "            self.encoder = self.encoder.roberta\n",
    "        else:\n",
    "            self.encoder = RobertaModel.from_pretrained(check_point).to(device)\n",
    "        for param in self.encoder.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(check_point)\n",
    "        self.config = AutoConfig.from_pretrained(check_point)\n",
    "\n",
    "        self.cmi = torch.nn.Parameter(torch.rand(1, requires_grad=True, device=device))\n",
    "        self.dmi = torch.nn.Parameter(torch.rand(1, requires_grad=True, device=device))\n",
    "\n",
    "        hidden_size = self.config.hidden_size\n",
    "        if decoder_type == 'BART':\n",
    "            self.decoder = BartDecoder.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder = BartForSequenceClassification.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder.load_state_dict(torch.load(dec_chk))\n",
    "            # self.decoder = self.decoder.model.decoder\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(hidden_size, hidden_size, 1, batch_first=True)\n",
    "\n",
    "        self.decoder_start_token_id = 2\n",
    "        self.lm_head = nn.Linear(hidden_size, self.config.vocab_size)\n",
    "        # self.target_labeler = FCLayer(hidden_size,num_targets,0.2)\n",
    "        self.lm_loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.x_sigma = x_sigma\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gamma = gamma\n",
    "        self.compressor = nn.Linear(2*hidden_size,hidden_size)\n",
    "\n",
    "        self.dropout_rate = params1.classifier_dropout\n",
    "\n",
    "        #Prepare the disentanglement modules.\n",
    "        self.continuous_inference = nn.Sequential()\n",
    "        self.disc_latent_inference = nn.Sequential()\n",
    "        conti_mean_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                           latent_dim=hidden_size,\n",
    "                                           disc_variable=False)\n",
    "        conti_logsigma_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                               latent_dim=hidden_size,\n",
    "                                               disc_variable=False)\n",
    "        self.continuous_inference.add_module(\"mean\", conti_mean_inf_module)\n",
    "        self.continuous_inference.add_module(\"log_sigma\", conti_logsigma_inf_module)\n",
    "\n",
    "        conti_mean_inf_module_t = _Inference(num_input_channels=hidden_size,\n",
    "                                           latent_dim=hidden_size,\n",
    "                                           disc_variable=False)\n",
    "        conti_logsigma_inf_module_t = _Inference(num_input_channels=hidden_size,\n",
    "                                               latent_dim=hidden_size,\n",
    "                                               disc_variable=False)\n",
    "        self.disc_latent_inference.add_module(\"mean\", conti_mean_inf_module_t)\n",
    "        self.disc_latent_inference.add_module(\"log_sigma\", conti_logsigma_inf_module_t)\n",
    "        # self._disc_latent_dim = num_targets\n",
    "        # dic_inf = _Inference(num_input_channels=hidden_size, latent_dim=self._disc_latent_dim,\n",
    "        #                      disc_variable=True)\n",
    "        # self.disc_latent_inference = dic_inf\n",
    "        sample = Sample(temperature=self.temperature)\n",
    "        self.sample = sample\n",
    "\n",
    "        self.kl_beta_c = beta_c\n",
    "        self.kl_beta_d = beta_d\n",
    "\n",
    "        #self.disc_log_prior_param = torch.log(torch.tensor([1 / self._disc_latent_dim for i in range(self._disc_latent_dim)]).view(1, -1).float().cuda())\n",
    "\n",
    "\n",
    "        #Reconstructor\n",
    "        self.reconstructor = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = hidden_size)\n",
    "        #Hate Classifier\n",
    "        self.hate_classifier = nn.Sequential(\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size, num_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        #Target Classifier\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size,num_targets)\n",
    "        )\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def get_contrastive_loss(self,embeddings,labels):\n",
    "      sf = nn.Softmax(dim=1)\n",
    "      softmax_labels = sf(labels)\n",
    "      print(\"LABELS: \",softmax_labels)\n",
    "      max_indices = torch.argmax(softmax_labels, dim=1)\n",
    "      weights = (max_indices.unsqueeze(1) == max_indices.unsqueeze(0)).float()\n",
    "      print(\"WEIGHTS:\",weights)\n",
    "      tensor_expanded = embeddings.unsqueeze(1) - embeddings.unsqueeze(0)\n",
    "      distance_matrix = torch.sqrt(torch.sum(tensor_expanded ** 2, dim=-1))/(embeddings.shape[1])\n",
    "      loss = torch.mean(weights * distance_matrix.pow(2) + (1 - weights) * torch.clamp_min(self.gamma - distance_matrix, 0).pow(2),axis=1)\n",
    "      return torch.mean(loss)\n",
    "\n",
    "    def get_lm_loss(self, logits, labels, masks):\n",
    "        '''Get the utterance reconstruction loss.'''\n",
    "        # labels = labels.float()\n",
    "        loss = self.lm_loss_fn(logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        masked_loss = loss * masks.view(-1)\n",
    "        return torch.mean(masked_loss)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, mask, decoder_inputs, decoder_masks, decoder_labels, vad_labels, labels):\n",
    "        \"\"\"\n",
    "        :param inputs: The input of PLM. Dim: [B, seq_len]\n",
    "        :param mask: The mask for input x. Dim: [B, seq_len]\n",
    "        \"\"\"\n",
    "        '''decoder_input_ids = shift_tokens_right(\n",
    "            x, self.config.pad_token_id, self.decoder_start_token_id\n",
    "        )'''\n",
    "        x = self.encoder(inputs, attention_mask=mask)[0]\n",
    "        x = x[:, 0, :].squeeze(1)\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        #Get the latent variables.\n",
    "        norm_mean = self.continuous_inference.mean(x)\n",
    "        norm_log_sigma = self.continuous_inference.log_sigma(x)\n",
    "        norm_mean_t = self.disc_latent_inference.mean(x)\n",
    "        norm_log_sigma_t = self.disc_latent_inference.log_sigma(x)\n",
    "        latent_sample = self.sample(norm_mean, norm_log_sigma, norm_mean_t,norm_log_sigma_t)\n",
    "        t_sample = self.sample._sample_norm(norm_mean_t, norm_log_sigma_t)\n",
    "        t_logits = self.target_classifier(t_sample)\n",
    "        con_loss = self.get_contrastive_loss(t_sample,t_logits)\n",
    "        print(con_loss)\n",
    "\n",
    "        latent_sample = torch.squeeze(latent_sample)\n",
    "        print(latent_sample.shape)\n",
    "        latent_sample = self.compressor(latent_sample)\n",
    "        decoder_hidden = self.reconstructor(latent_sample)\n",
    "\n",
    "        if self.decoder_type == 'BART':\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_inputs,\n",
    "                attention_mask=decoder_masks,\n",
    "                encoder_hidden_states=decoder_hidden.unsqueeze(1))\n",
    "            lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        else:\n",
    "            input_embeddings = self.encoder.embeddings(decoder_inputs)\n",
    "            h = decoder_hidden.unsqueeze(0)\n",
    "            decoder_outputs, (_, _) = self.decoder(input_embeddings, (h, torch.zeros(h.shape).to(self.device)))\n",
    "            lm_logits = self.lm_head(decoder_outputs)\n",
    "\n",
    "        # reconstruct_loss = F.mse_loss(lm_logits, x, reduction=\"mean\") / (2 * self.batch_size * (self.x_sigma ** 2))\n",
    "        # preds = F.gumbel_softmax(lm_logits, tau=1, hard=False)\n",
    "        reconstruct_loss = self.get_lm_loss(lm_logits, decoder_labels, decoder_masks)#/ (2 * self.batch_size * (self.x_sigma ** 2))\n",
    "\n",
    "\n",
    "        # calculate latent space KL divergence\n",
    "        z_mean_sq = norm_mean * norm_mean\n",
    "        z_log_sigma_sq = 2 * norm_log_sigma\n",
    "        z_sigma_sq = torch.exp(z_log_sigma_sq)\n",
    "        continuous_kl_loss = 0.5 * torch.sum(z_mean_sq + z_sigma_sq - z_log_sigma_sq - 1) / self.batch_size\n",
    "        # notice here we duplicate the 0.5 by each part\n",
    "        # disc param : log(a1),...,log(an) type\n",
    "        # disc_kl_loss = torch.sum(torch.exp(disc_log_alpha) * (disc_log_alpha - self.disc_log_prior_param)) / self.batch_size\n",
    "\n",
    "        prior_kl_loss_l = self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi) #+ self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi)\n",
    "        elbo_loss_l = reconstruct_loss + prior_kl_loss_l\n",
    "\n",
    "        #Calculate the classification loss.\n",
    "        hate_sample = self.sample._sample_norm(norm_mean, norm_log_sigma)\n",
    "        #target_sample = self.sample._sample_gumbel_softmax(disc_log_alpha)\n",
    "\n",
    "        # hate_sample = F.dropout(hate_sample, p=self.dropout_rate, training=self.training)\n",
    "        # target_sample = self.get_vad_loss(target_sample,vad_labels)\n",
    "        hate_logits = self.hate_classifier(hate_sample)\n",
    "\n",
    "        # target_labels = torch.zeros_like(vad_labels).float().cuda()\n",
    "        # target_labels[target_logits] = 1\n",
    "\n",
    "        # print(\"Reconstruction loss\",reconstruct_loss,\"KL Continous Loss\",self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi),\"KL Discrete loss\",self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi),end=\",\")\n",
    "        return elbo_loss_l[0], hate_logits, con_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# gpt_data = pd.read_csv(\"D:/Hate Speech/Preprocessed Datasets/YouTube/GPT4Analysis.csv\")\n",
    "# true_data_tr = pd.read_csv(\"D:/Hate Speech/Preprocessed Datasets/YouTube/youtube_Numeric.csv\")\n",
    "# true_data_te = pd.read_csv(\"D:/Hate Speech/Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced_test.csv\")\n",
    "\n",
    "gpt_data = pd.read_csv(\"D:/Hate Speech/Preprocessed Datasets/GAB/GPT4Analysis.csv\")\n",
    "true_data_tr = pd.read_csv(\"D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv\")\n",
    "true_data_te = pd.read_csv(\"D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv\")\n",
    "\n",
    "\n",
    "data1 = pd.merge(gpt_data,true_data_tr,on=['text'])\n",
    "data2 = pd.merge(gpt_data,true_data_te,on=['text'])\n",
    "data1.shape,true_data_tr.shape,gpt_data.shape,true_data_te.shape,data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_dict = {\"Race\":0,\"Religion\":1,\"Sexuality and Sexual Preferences\":2,\"Gender\":3,\"Immigration Status\":4,\"Nationality\":5,\"Ableness/Disability\":6,\"Class\":7}\n",
    "\n",
    "data1['pred_target1'] = data1['pred_target'].map(target_label_dict)\n",
    "data1.dropna(subset=['pred_target1'], inplace=True)\n",
    "data1['pred_target1'].astype('int32')\n",
    "data1 = data1[['text','label','pred_target1','target_y']]\n",
    "data1.columns = ['text','label','target','tgt_og']\n",
    "data1.drop_duplicates(inplace=True)\n",
    "data1.shape\n",
    "data1.to_csv(\"D:/Hate Speech/Preprocessed Datasets/GAB/gab_hx_Numeric_train_WS.csv\",index=False)\n",
    "\n",
    "\n",
    "data2['pred_target1'] = data2['pred_target'].map(target_label_dict)\n",
    "data2.dropna(subset=['pred_target1'], inplace=True)\n",
    "data2['pred_target1'].astype('int32')\n",
    "data2 = data2[['text','label','pred_target1','target_y']]\n",
    "data2.columns = ['text','label','target','tgt_og']\n",
    "data2.drop_duplicates(inplace=True)\n",
    "data2.shape\n",
    "data2.to_csv(\"D:/Hate Speech/Preprocessed Datasets/GAB/gab_hx_Numeric_test_WS.csv\",index=False)\n",
    "data1.shape,data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a6511c335c26d02572349af57853ebfcc20500c776be641a45dee87cf591594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
